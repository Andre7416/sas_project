{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fa98fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2fecc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_add = pd.read_csv('train_add.csv')\n",
    "test_add = pd.read_csv('test_add.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f491b4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_dataframe(df, date_column):\n",
    "    # Copy the original dataframe to avoid modifying the original\n",
    "    transformed_df = df.copy()\n",
    "\n",
    "    # Convert the date column to datetime type\n",
    "    transformed_df[date_column] = pd.to_datetime(transformed_df[date_column])\n",
    "\n",
    "    # Extract day, month, and year from the date column\n",
    "    transformed_df['day'] = transformed_df[date_column].dt.day\n",
    "    transformed_df['month'] = transformed_df[date_column].dt.month\n",
    "    transformed_df['year'] = transformed_df[date_column].dt.year\n",
    "\n",
    "    # Add day of the week column\n",
    "    # transformed_df['day_of_week'] = transformed_df[date_column].dt.weekday\n",
    "\n",
    "    # Remove the original date column\n",
    "    transformed_df.drop(date_column, axis=1, inplace=True)\n",
    "\n",
    "    return transformed_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59be5174",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = transform_dataframe(train_add, 'period_dt')\n",
    "test_data = transform_dataframe(test_add, 'period_dt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90aea70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def transform_dt_df(df):\n",
    "    # Copy the original dataframe to avoid modifying the original\n",
    "    transformed_df = df.copy()\n",
    "\n",
    "    # Specify the columns to transform\n",
    "    columns_to_transform = [\n",
    "                            'VALID_FROM_DTTM_y']\n",
    "\n",
    "    # Transform the specified columns\n",
    "    for column in columns_to_transform:\n",
    "        transformed_df[column] = pd.to_datetime(transformed_df[column], format=\"%d%b%Y:%H:%M:%S\")\n",
    "\n",
    "        # Split date into day, month, and year\n",
    "        transformed_df[column + '_day'] = transformed_df[column].dt.day\n",
    "        transformed_df[column + '_month'] = transformed_df[column].dt.month\n",
    "        transformed_df[column + '_year'] = transformed_df[column].dt.year\n",
    "\n",
    "        # Split time into separate columns\n",
    "        transformed_df[column + '_hour'] = transformed_df[column].dt.hour\n",
    "        transformed_df[column + '_minute'] = transformed_df[column].dt.minute\n",
    "        transformed_df[column + '_second'] = transformed_df[column].dt.second\n",
    "\n",
    "        # Add weekday column\n",
    "        transformed_df[column + '_weekday'] = transformed_df[column].dt.weekday\n",
    "\n",
    "        # Ensure dates are no later than the year 2260\n",
    "        transformed_df.loc[transformed_df[column + '_year'] > 2260, [column + '_day', column + '_month', column + '_year']] = [1, 1, 2260]\n",
    "\n",
    "    # Remove the original string columns\n",
    "    transformed_df.drop(columns_to_transform, axis=1, inplace=True)\n",
    "\n",
    "    return transformed_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56543bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = transform_dt_df(train_data)\n",
    "test_data = transform_dt_df(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2729df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_df(df, D):\n",
    "    # Convert day, month, and year columns to datetime format\n",
    "    df['date'] = pd.to_datetime(df[['day', 'month', 'year']])\n",
    "    df.sort_values(by='date', inplace=True)\n",
    "    # Group the DataFrame by product_id and find the first occurrence\n",
    "    first_occurrence = df.groupby(by=['location_id', 'product_id'])['date'].first()\n",
    "    first_occurrence = pd.DataFrame(first_occurrence)\n",
    "    first_occurrence.reset_index(inplace=True)\n",
    "    first_occurrence.rename(columns={'date':'first_date'}, inplace=True)\n",
    "    # Merge the first occurrence back to the original DataFrame\n",
    "    df = df.merge(first_occurrence, on=['location_id','product_id'], suffixes=('', '_first'))\n",
    "\n",
    "    # Calculate the difference in days between subsequent occurrences and the first occurrence\n",
    "    df['date_diff'] = (df['date'] - df['first_date']).dt.days\n",
    "\n",
    "    # Filter out the products where the date difference is greater than D\n",
    "    filtered_df = df[df['date_diff'] <= D].copy()\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    filtered_df.drop(['first_date', 'date_diff'], axis=1, inplace=True)\n",
    "\n",
    "    return filtered_df\n",
    "\n",
    "train_data = filter_df(train_data, 31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f58de530",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ef6cb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# добавим процент от начальной цены\n",
    "train_data.loc[:, 'disc_percent'] = train_data['PRICE_AFTER_DISC'] / train_data['PRICE_REGULAR']\n",
    "test_data.loc[:, 'disc_percent'] = test_data['PRICE_AFTER_DISC'] / test_data['PRICE_REGULAR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "daf341f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Средние значения ответа\n",
    "group_all = train_data[['month', 'location_id', 'demand']].groupby(by=['month', 'location_id'], as_index=False).mean()\n",
    "\n",
    "group_date = train_data[['month', 'demand']].groupby(by=['month'], as_index=False).mean()\n",
    "train_data = train_data.merge(group_all, on=['month', 'location_id'], how='left', suffixes=('', '_all'))\n",
    "test_data = test_data.merge(group_all, on=['month', 'location_id'], how='left', suffixes=('', '_all'))\n",
    "train_data = train_data.merge(group_date, on=['month'], how='left', suffixes=('', '_new'))\n",
    "test_data = test_data.merge(group_date, on=['month'], how='left', suffixes=('', '_new'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65573f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.sort_values(by=['year', 'month', 'day'])\n",
    "tmp = train_data.groupby(by=['location_id', 'product_id'], as_index=False).cumcount() + 1\n",
    "train_data.loc[:, 'number_of_week'] = tmp\n",
    "test_data = test_data.sort_values(by=['year', 'month', 'day'])\n",
    "tmp = test_data.groupby(by=['location_id', 'product_id'], as_index=False).cumcount() + 1\n",
    "test_data.loc[:, 'number_of_week'] = tmp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35e683ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.drop(columns=['date'], inplace=True)\n",
    "train_data.drop(columns=['product_id'], inplace=True)\n",
    "test_data.drop(columns=['product_id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7c4f9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_features = train_data.columns[(train_data.dtypes == 'object')].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6c04f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "def demand_bucketing(df_column, closest_points):\n",
    "    column = list(df_column)\n",
    "    ans = []\n",
    "    for elem in column:\n",
    "        if elem > 5:\n",
    "            ans.append(-1)\n",
    "            continue\n",
    "        min_val = 100500\n",
    "        cor_point = 0\n",
    "        for it2, point in enumerate(closest_points):\n",
    "            if abs(point - elem) < min_val:\n",
    "                min_val = abs(point - elem)\n",
    "                cor_point = point\n",
    "        ans.append(cor_point)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "823311da",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.loc[:, 'demand_class'] = demand_bucketing(train_data['demand'], [0, 0.3, 0.5, 0.7, 1, 1.5, 2, 2.5, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60d39cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "bestTest = 0.7706456456\n",
      "bestIteration = 281\n",
      "\n",
      "0:\tloss: 0.7706456\tbest: 0.7706456 (0)\ttotal: 3m 50s\tremaining: 1h 5m 10s\n",
      "\n",
      "bestTest = 0.7787787788\n",
      "bestIteration = 342\n",
      "\n",
      "1:\tloss: 0.7787788\tbest: 0.7787788 (1)\ttotal: 7m 56s\tremaining: 1h 3m 30s\n",
      "\n",
      "bestTest = 0.7831581582\n",
      "bestIteration = 347\n",
      "\n",
      "2:\tloss: 0.7831582\tbest: 0.7831582 (2)\ttotal: 12m 5s\tremaining: 1h 29s\n",
      "\n",
      "bestTest = 0.7611361361\n",
      "bestIteration = 68\n",
      "\n",
      "3:\tloss: 0.7611361\tbest: 0.7831582 (2)\ttotal: 16m 5s\tremaining: 56m 19s\n",
      "\n",
      "bestTest = 0.765015015\n",
      "bestIteration = 348\n",
      "\n",
      "4:\tloss: 0.7650150\tbest: 0.7831582 (2)\ttotal: 21m 48s\tremaining: 56m 42s\n",
      "\n",
      "bestTest = 0.7829079079\n",
      "bestIteration = 348\n",
      "\n",
      "5:\tloss: 0.7829079\tbest: 0.7831582 (2)\ttotal: 27m 45s\tremaining: 55m 30s\n",
      "\n",
      "bestTest = 0.7882882883\n",
      "bestIteration = 323\n",
      "\n",
      "6:\tloss: 0.7882883\tbest: 0.7882883 (6)\ttotal: 33m 45s\tremaining: 53m 3s\n",
      "\n",
      "bestTest = 0.7881631632\n",
      "bestIteration = 336\n",
      "\n",
      "7:\tloss: 0.7881632\tbest: 0.7882883 (6)\ttotal: 39m 54s\tremaining: 49m 53s\n",
      "\n",
      "bestTest = 0.733983984\n",
      "bestIteration = 7\n",
      "\n",
      "8:\tloss: 0.7339840\tbest: 0.7882883 (6)\ttotal: 46m 1s\tremaining: 46m 1s\n",
      "\n",
      "bestTest = 0.7738988989\n",
      "bestIteration = 343\n",
      "\n",
      "9:\tloss: 0.7738989\tbest: 0.7882883 (6)\ttotal: 58m 2s\tremaining: 46m 26s\n",
      "\n",
      "bestTest = 0.7945445445\n",
      "bestIteration = 332\n",
      "\n",
      "10:\tloss: 0.7945445\tbest: 0.7945445 (10)\ttotal: 1h 11m 31s\tremaining: 45m 30s\n",
      "\n",
      "bestTest = 0.7989239239\n",
      "bestIteration = 324\n",
      "\n",
      "11:\tloss: 0.7989239\tbest: 0.7989239 (11)\ttotal: 1h 25m 12s\tremaining: 42m 36s\n",
      "\n",
      "bestTest = 0.7974224224\n",
      "bestIteration = 345\n",
      "\n",
      "12:\tloss: 0.7974224\tbest: 0.7989239 (11)\ttotal: 1h 39m 1s\tremaining: 38m 5s\n",
      "\n",
      "bestTest = 0.7799049049\n",
      "bestIteration = 252\n",
      "\n",
      "13:\tloss: 0.7799049\tbest: 0.7989239 (11)\ttotal: 1h 53m 55s\tremaining: 32m 32s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/wq/k2_v35311db4bsb1x7501n4w0000gn/T/ipykernel_30885/1740978649.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m                       }\n\u001b[1;32m      9\u001b[0m \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCatBoostClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msilent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_metric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Accuracy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m350\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmodel_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandomized_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m18\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/catboost/core.py\u001b[0m in \u001b[0;36mrandomized_search\u001b[0;34m(self, param_distributions, X, y, cv, n_iter, partition_random_seed, calc_cv_statistics, search_by_train_test_split, refit, shuffle, stratified, train_size, verbose, plot, plot_file, log_cout, log_cerr)\u001b[0m\n\u001b[1;32m   4228\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Parameter grid value is not iterable and do not have \\'rvs\\' method (key={!r}, value={!r})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_distributions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4230\u001b[0;31m         return self._tune_hyperparams(\n\u001b[0m\u001b[1;32m   4231\u001b[0m             \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4232\u001b[0m             \u001b[0mpartition_random_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartition_random_seed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcalc_cv_statistics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcalc_cv_statistics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/catboost/core.py\u001b[0m in \u001b[0;36m_tune_hyperparams\u001b[0;34m(self, param_grid, X, y, cv, n_iter, partition_random_seed, calc_cv_statistics, search_by_train_test_split, refit, shuffle, stratified, train_size, verbose, plot, plot_file, log_cout, log_cerr)\u001b[0m\n\u001b[1;32m   4014\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4015\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mlog_fixup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_cout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_cerr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Hyperparameters search plot'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_get_train_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4016\u001b[0;31m             cv_result = self._object._tune_hyperparams(\n\u001b[0m\u001b[1;32m   4017\u001b[0m                 \u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train_pool\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4018\u001b[0m                 \u001b[0mfold_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartition_random_seed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstratified\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._CatBoost._tune_hyperparams\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._CatBoost._tune_hyperparams\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier, Pool\n",
    "X, y = train_data.drop(columns=['demand', 'demand_class']), train_data['demand_class']\n",
    "data = Pool(X, y, cat_features=text_features)\n",
    "param_distributions = {'learning_rate': [0.03, 0.1, 0.2, 0.4, 0.6],\n",
    "                        'depth': [3, 4, 6, 7],\n",
    "                        'l2_leaf_reg': np.arange(0.03, 4, 4),\n",
    "                       \n",
    "                      }\n",
    "model_class = CatBoostClassifier(silent=True, random_seed=42, eval_metric='Accuracy', n_estimators=350, cat_features=text_features)\n",
    "model_class.randomized_search(param_distributions, data, n_iter=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793aea9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_class.predict(test_data.drop(columns=['demand']))\n",
    "test_data.loc[:, 'demand'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ac2a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(20, 14))\n",
    "sns.histplot(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8445f567",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.loc[test_data['demand'] == -1, 'demand'] = 3\n",
    "test_data.loc[test_data['demand'] < 1, 'demand'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458cffb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[['id', 'demand']].to_csv(\"ans_Andrey.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
